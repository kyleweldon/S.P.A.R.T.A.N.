{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b28c29-abf6-477f-abf9-b68504661dd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T20:22:29.230912Z",
     "start_time": "2024-06-30T20:22:26.734229Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Supress TensorFlow messages\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input,Dense, Concatenate, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import shap"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Versions used:**\n",
    "* numpy==1.26.4\n",
    "* pandas==2.2.2\n",
    "* scikit-learn==1.4.2\n",
    "* tensorflow==2.16.1\n",
    "* shap==0.46.0"
   ],
   "id": "612a438b78896fcf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Filtering raw data given:** \n",
    "\n",
    "The data that was given had incomplete data in a '.xlsx' file. The code below is what was used to remove all of the uncomplete samples and save the result as a '.csv' file."
   ],
   "id": "3235c08a831c30d8"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feb90d5cfa9d70cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T21:00:25.126260Z",
     "start_time": "2024-06-30T21:00:24.244885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to 'Data/FilteredData.csv' successfully.\n",
      "There are 881 samples in the cleaned data.\n"
     ]
    }
   ],
   "source": [
    "def filter_data(excel_file, output_csv):\n",
    "    try:\n",
    "        df = pd.read_excel(excel_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{excel_file}' was not found.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while reading '{excel_file}': {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Filter rows based on completeness (non-empty cells)\n",
    "    complete_rows = []\n",
    "    for index, row in df.iterrows():\n",
    "        if is_row_complete(row):\n",
    "            complete_rows.append(row)\n",
    "\n",
    "    cleaned_df = pd.DataFrame(complete_rows, columns=df.columns)\n",
    "\n",
    "    # Step 3: Save the cleaned data to a CSV file\n",
    "    try:\n",
    "        cleaned_df.to_csv(output_csv, index=False)\n",
    "        print(f\"Cleaned data saved to '{output_csv}' successfully.\")\n",
    "        print(f\"There are {len(cleaned_df)} samples in the cleaned data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while saving to '{output_csv}': {str(e)}\")\n",
    "        return\n",
    "\n",
    "def is_row_complete(row):\n",
    "    for cell in row:\n",
    "        # Check if cell is NaN or empty (after stripping whitespace)\n",
    "        if pd.isna(cell) or str(cell).strip() == '':\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "filter_data('Data/RawData.xlsx', 'Data/FilteredData.csv')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **How to use the data:**\n",
    " There are 10 different 'senerios' or 'decisions' made by each sample (each sample represents one person). When making a decision they were able to choose between 0-10 based on how sure they are. This gives 11 possible choices per situation per sample. Given the fact there are only 881 samples attempting to accruetly predict 11 possible choices will likely not be accurate due to the limited data. To account for this The decisions are going to be split into three catagories. Anyone that chose a 0, 1, 2, or 3 will be a part of catagory one. Anyone that chose either 4, 5, or 6 will be a part of catagory two and anyone that chose 7, 8, 9, or 10 will be a part of catagory three. This gives a 4-3-4 catigorical split. Below is the code that completes this. "
   ],
   "id": "377b4b7dd6f48fe2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T22:32:50.348671Z",
     "start_time": "2024-06-30T22:32:50.316414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('Data/FilteredData.csv')\n",
    "# Column tites for all the output data\n",
    "output_columns = ['Scenario 1 ',\n",
    "                  'Unnamed: 40',\n",
    "                  'Scenario 2 ',\n",
    "                  'Unnamed: 42',\n",
    "                  'Scenario 3 ',\n",
    "                  'Unnamed: 44',\n",
    "                  'Scenario 4',\n",
    "                  'Unnamed: 46',\n",
    "                  'Scenario 5 ',\n",
    "                  'Unnamed: 48']\n",
    "\n",
    "multi_output_df = df[output_columns]\n",
    "all_Y_values = multi_output_df.to_numpy()\n",
    "\n",
    "catigorized_Y_values = []\n",
    "for sample in all_Y_values:\n",
    "    temp = []\n",
    "    for cell in sample:\n",
    "        if cell <= 3:\n",
    "            temp.append(0)\n",
    "        elif cell <= 6:\n",
    "            temp.append(1)\n",
    "        else:\n",
    "            temp.append(2)\n",
    "    catigorized_Y_values.append(temp)\n",
    "    \n",
    "catigorized_Y_values = np.array(catigorized_Y_values)\n",
    "Y_values_catigorized = to_categorical(catigorized_Y_values)\n",
    "\n",
    "print(Y_values_catigorized)\n"
   ],
   "id": "ccf75147951b9c03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 1.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 1.]\n",
      "  ...\n",
      "  [0. 1. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 1. 0.]]\n",
      "\n",
      " [[0. 0. 1.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 1.]\n",
      "  ...\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 1.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 1.]\n",
      "  ...\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 1. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 1.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 1.]\n",
      "  ...\n",
      "  [0. 0. 1.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 1.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 1. 0.]\n",
      "  ...\n",
      "  [0. 1. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 1. 0.]]\n",
      "\n",
      " [[0. 0. 1.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 1.]\n",
      "  ...\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 1.]]]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Splitting into training and validating:**\n",
    "Before this data can be used to train a model it first needs to be split into traning and validating data. Below is the code that does that. The first 800 samples (people) are going to be used train the model while the last 81 are going to be for validation."
   ],
   "id": "815c9e8cc1512938"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6abfd1fb814cade7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
