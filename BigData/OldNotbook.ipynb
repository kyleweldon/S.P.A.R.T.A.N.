{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ***By Kyle Weldon*** ",
   "id": "9c6459a68a06a97a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T17:44:20.370376Z",
     "start_time": "2024-07-16T17:44:20.362777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "# Supress TensorFlow messages\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input,Dense, Concatenate, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import shap\n",
    "\n",
    "import sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('NumPy version:', np.__version__)\n",
    "print('Pandas version:', pd.__version__)\n",
    "print('Scikit-learn version:', sklearn.__version__)\n",
    "print('Tensorflow version:', tf.__version__)\n",
    "print('shap version:', shap.__version__)"
   ],
   "id": "96a853cb07d358fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n",
      "Pandas version: 2.2.2\n",
      "Scikit-learn version: 1.5.1\n",
      "Tensorflow version: 2.17.0\n",
      "shap version: 0.46.0\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Filtering raw data given:** \n",
    "\n",
    "The data that was given had incomplete data in a '.xlsx' file. The code below is what was used to remove all of the uncomplete samples and save the result as a '.csv' file."
   ],
   "id": "3235c08a831c30d8"
  },
  {
   "cell_type": "code",
   "id": "feb90d5cfa9d70cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T17:46:00.176552Z",
     "start_time": "2024-07-16T17:45:58.643700Z"
    }
   },
   "source": [
    "def filter_data(excel_file, output_csv):\n",
    "    try:\n",
    "        df = pd.read_excel(excel_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{excel_file}' was not found.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while reading '{excel_file}': {str(e)}\")\n",
    "        return\n",
    "\n",
    "    complete_rows = []\n",
    "    for index, row in df.iterrows():\n",
    "        if is_row_complete(row):\n",
    "            complete_rows.append(row)\n",
    "\n",
    "    cleaned_df = pd.DataFrame(complete_rows, columns=df.columns)\n",
    "\n",
    "    try:\n",
    "        cleaned_df.to_csv(output_csv, index=False)\n",
    "        print(f\"Cleaned data saved to '{output_csv}' successfully.\")\n",
    "        print(f\"There are {len(cleaned_df)} samples in the cleaned data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while saving to '{output_csv}': {str(e)}\")\n",
    "        return\n",
    "\n",
    "def is_row_complete(row):\n",
    "    for cell in row:\n",
    "        if pd.isna(cell) or str(cell).strip() == '':\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "filter_data('Data/RawData.xlsx', 'Data/FilteredData.csv')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to 'Data/FilteredData.csv' successfully.\n",
      "There are 881 samples in the cleaned data.\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **How to use the data:**\n",
    " There are 10 different 'senerios' or 'decisions' made by each sample (each sample represents one person). When making a decision they were able to choose between 0-10 based on how sure they are. This gives 11 possible choices per situation per sample. Given the fact there are only 881 samples attempting to accruetly predict 11 possible choices will likely not be accurate due to the limited data. To account for this The decisions are going to be split into three catagories. Anyone that chose a 0, 1, 2, or 3 will be a part of catagory one. Anyone that chose either 4, 5, or 6 will be a part of catagory two and anyone that chose 7, 8, 9, or 10 will be a part of catagory three. This gives a 4-3-4 catigorical split. Below is the code that completes this. It is also essential to remember to split the data into training and validating data. "
   ],
   "id": "377b4b7dd6f48fe2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T17:46:07.999998Z",
     "start_time": "2024-07-16T17:46:07.962415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('Data/FilteredData.csv')\n",
    "# Column tites for all the output data\n",
    "output_columns = ['Scenario 1 ',\n",
    "                  'Unnamed: 40',\n",
    "                  'Scenario 2 ',\n",
    "                  'Unnamed: 42',\n",
    "                  'Scenario 3 ',\n",
    "                  'Unnamed: 44',\n",
    "                  'Scenario 4',\n",
    "                  'Unnamed: 46',\n",
    "                  'Scenario 5 ',\n",
    "                  'Unnamed: 48']\n",
    "\n",
    "def classiy_and_catigorize(column):\n",
    "    return to_categorical([0 if x <= 3 else 1 if x <= 6 else 2 for x in column])\n",
    "\n",
    "columns = df[output_columns].to_numpy().T # The 'T' is to transpose the array\n",
    "\n",
    "S1P1, S1P2, S2P1, S2P2, S3P1, S3P2, S4P1, S4P2, S5P1, S5P2 = [classiy_and_catigorize(col) for col in columns]\n",
    "all_situations = [S1P1, S1P2, S2P1, S2P2, S3P1, S3P2, S4P1, S4P2, S5P1, S5P2]\n",
    "\n",
    "for situation in all_situations:\n",
    "    print(situation.shape)"
   ],
   "id": "ccf75147951b9c03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(881, 3)\n",
      "(881, 3)\n",
      "(881, 3)\n",
      "(881, 3)\n",
      "(881, 3)\n",
      "(881, 3)\n",
      "(881, 3)\n",
      "(881, 3)\n",
      "(881, 3)\n",
      "(881, 3)\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Splitting into training and validating:**\n",
    "Before this data can be used to train a model it first needs to be split into traning and validating data. Below is the code that does that. The first 800 samples (people) are going to be used train the model while the last 81 are going to be for validation."
   ],
   "id": "815c9e8cc1512938"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T17:46:11.772109Z",
     "start_time": "2024-07-16T17:46:11.762242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split(array):\n",
    "    return array[:800], array[800:]\n",
    "\n",
    "S1P1_train, S1P1_val = split(S1P1)\n",
    "S1P2_train, S1P2_val = split(S1P2)\n",
    "S2P1_train, S2P1_val = split(S2P1)\n",
    "S2P2_train, S2P2_val = split(S2P2)\n",
    "S3P1_train, S3P1_val = split(S3P1)\n",
    "S3P2_train, S3P2_val = split(S3P2)\n",
    "S4P1_train, S4P1_val = split(S4P1)\n",
    "S4P2_train, S4P2_val = split(S4P2)\n",
    "S5P1_train, S5P1_val = split(S5P1)\n",
    "S5P2_train, S5P2_val = split(S5P2)\n",
    "\n",
    "print(f\"S1P1 training shape: {S1P1_train.shape}\")\n",
    "print(f\"Validation set length: {len(S1P1_val)}\")"
   ],
   "id": "6abfd1fb814cade7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1P1 training shape: (800, 3)\n",
      "Validation set length: 81\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Prepare input data:**\n",
    "Now that the output data is fully prepared and ready for training it is time to prepare the coresponding input data. Simalar to the multiple outputs there our also miltiple inputs. Below is the code to complete this."
   ],
   "id": "7e900f02d472b69a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T17:46:15.862299Z",
     "start_time": "2024-07-16T17:46:15.852382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Column titles used for this input set\n",
    "input_columns1 = ['MAx1', 'Max2', 'Max3']\n",
    "input_columns2 = ['Q105_1','Q105_2','Q105_3','Q105_4','Q105_5','Q105_6','Q105_7','Q105_8','Q105_9','Q105_10','Q105_11','Q105_12','Q105_13','Q105_14','Q105_15','Q105_16','Q105_17','Q105_18','Q105_19','Q105_20','Q105_21','Q105_22','Q105_23','Q105_24','Q105_25','Q105_26','Q105_27','Q105_28','Q105_29','Q105_30','Q105_31','Q105_32','Q105_33','Q105_34']\n",
    "\n",
    "input_df1 = df[input_columns1]\n",
    "input1_X_values = input_df1.to_numpy()\n",
    "input_df2 = df[input_columns2]\n",
    "input2_X_values = input_df2.to_numpy()\n",
    "\n",
    "layer1_X_train = input1_X_values[:800]\n",
    "layer1_X_val = input1_X_values[800:]\n",
    "layer2_X_train = input2_X_values[:800]\n",
    "layer2_X_val = input2_X_values[800:]\n",
    "\n",
    "print(f\"Layer1 training shape: {layer1_X_train.shape} -> validating shape: {layer1_X_val.shape}\")\n",
    "print(f\"Layer2 training shape: {layer2_X_train.shape} -> validating shape: {layer2_X_val.shape}\")"
   ],
   "id": "3895df9c28f82fb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer1 training shape: (800, 3) -> validating shape: (81, 3)\n",
      "Layer2 training shape: (800, 34) -> validating shape: (81, 34)\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Building model archetecture:**\n",
    "First instinct is build a deep neural network. Because this initial model will predict all 10 different decisions for each sample a multi-output model is need. There was some adjustment needed to the output layers so they are the correct shape. The functional API from Keras is used for this."
   ],
   "id": "5159403838d8334d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:23:17.670693Z",
     "start_time": "2024-07-16T19:23:17.422503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input1 = Input(shape=(3,))\n",
    "\n",
    "hidden1_input1 = Dense(256, activation='relu', kernel_regularizer=l2(0.01), name='DenseOne_Input1')(input1)\n",
    "hidden2_input1 = Dense(128, activation='relu', kernel_regularizer=l2(0.01), name='DenseTwo_Input1')(hidden1_input1)\n",
    "hidden3_input1 = Dense(64, activation='relu', kernel_regularizer=l2(0.01), name='DenseThree_Input1')(hidden2_input1)\n",
    "hidden4_input1 = Dense(32, activation='relu', kernel_regularizer=l2(0.01), name='DenseFour_Input1')(hidden3_input1)\n",
    "hidden5_input1 = Dense(16, activation='relu', kernel_regularizer=l2(0.01), name='DenseFive_Input1')(hidden4_input1)\n",
    "\n",
    "# input2 = Input(shape=(34,), name='InputLayer2')\n",
    "# \n",
    "# hidden1_input2 = Dense(256, activation='relu', kernel_regularizer=l2(0.01), name='DenseOne_Input2')(input2)\n",
    "# hidden2_input2 = Dense(128, activation='relu', kernel_regularizer=l2(0.01), name='DenseTwo_Input2')(hidden1_input2)\n",
    "# hidden3_input2 = Dense(64, activation='relu', kernel_regularizer=l2(0.01), name='DenseThree_Input2')(hidden2_input2)\n",
    "# hidden4_input2 = Dense(32, activation='relu', kernel_regularizer=l2(0.01), name='DenseFour_Input2')(hidden3_input2)\n",
    "# hidden5_input2 = Dense(16, activation='relu', kernel_regularizer=l2(0.01), name='DenseFive_Input2')(hidden4_input2)\n",
    "# \n",
    "# concatenated = Concatenate(name='ConcatinatedInput')([hidden5_input1, hidden5_input2])\n",
    "\n",
    "def create_output_layer(name, input_layer):\n",
    "    return Dense(3, activation='softmax', name=name)(input_layer)\n",
    "def itterate_situations_and_parts(num_itts=10):\n",
    "    scenerio = 1\n",
    "    sittos = []\n",
    "    for i in range(num_itts):\n",
    "        part = 1 if i % 2 == 0 else 2\n",
    "        sittos.append(f\"S{scenerio}P{part}\")\n",
    "        if part == 2: scenerio += 1\n",
    "    return sittos\n",
    "\n",
    "outputs = [create_output_layer(name, concatenated) for name in itterate_situations_and_parts()]\n",
    "\n",
    "model = Model(inputs=input1, outputs=outputs, name='CustomizedDeepNeuralNetwork')\n",
    "\n",
    "metrics = ['accuracy'] * 10  \n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=metrics)\n",
    "\n",
    "print(model.summary())"
   ],
   "id": "eab12da1c0c9079d",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'Dimension' and 'int', please use // instead",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[78], line 40\u001B[0m\n\u001B[0;32m     34\u001B[0m metrics \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m10\u001B[39m  \n\u001B[0;32m     36\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategorical_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     37\u001B[0m               optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     38\u001B[0m               metrics\u001B[38;5;241m=\u001B[39mmetrics)\n\u001B[1;32m---> 40\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msummary\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32mC:\\KyleWeldon\\Projects\\ThinkTank\\S.P.A.R.T.A.N\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32mC:\\KyleWeldon\\Projects\\ThinkTank\\S.P.A.R.T.A.N\\venv\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:565\u001B[0m, in \u001B[0;36mDimension.__truediv__\u001B[1;34m(self, other)\u001B[0m\n\u001B[0;32m    552\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__truediv__\u001B[39m(\u001B[38;5;28mself\u001B[39m, other):\n\u001B[0;32m    553\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Use `__floordiv__` via `x // y` instead.\u001B[39;00m\n\u001B[0;32m    554\u001B[0m \n\u001B[0;32m    555\u001B[0m \u001B[38;5;124;03m  This function exists only to have a better error message. Instead of:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    563\u001B[0m \u001B[38;5;124;03m    TypeError.\u001B[39;00m\n\u001B[0;32m    564\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[1;32m--> 565\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munsupported operand type(s) for /: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDimension\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    566\u001B[0m                   \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mplease use // instead\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mtype\u001B[39m(other)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m))\n",
      "\u001B[1;31mTypeError\u001B[0m: unsupported operand type(s) for /: 'Dimension' and 'int', please use // instead"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Training the model:**\n",
    "Now that the model has been built and compiled it is ready for the training data that has been previously prepared. Below is the code for training the model."
   ],
   "id": "befcdba0f1fa299b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.fit(layer1_X_train, [S1P1_train, S1P2_train, S2P1_train,\n",
    "                    S2P2_train, S3P1_train, S3P2_train, S4P1_train,\n",
    "                    S4P2_train, S5P1_train, S5P2_train],\n",
    "          epochs=10,\n",
    "          batch_size=32,\n",
    "          validation_data=(layer1_X_val, [S1P1_val, S1P2_val, S2P1_val,\n",
    "                                                          S2P2_val, S3P1_val, S3P2_val, S4P1_val,\n",
    "                                                          S4P2_val, S5P1_val, S5P2_val]))"
   ],
   "id": "f0cca0f50fcf7142",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Attempts made to improve output:**\n",
    "The loss calculation is high as seen in this output. The closer a loss is to 0 the better, but the above calculations are far from 0. Simplifying the model's complexity, adjusting the L2 regularzation penalties, and changing the number of epochs but none seemed to have any effect on the loss of the model.\n"
   ],
   "id": "5f86a2a9523d48b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Explaining the output with SHAP (SHapley Additive exPlanations):**\n",
   "id": "5ee6c88ea90efd09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "explainer = shap.DeepExplainer(model,[layer1_X_train, layer2_X_train])"
   ],
   "id": "be8040142ced5995",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Better understanding the data itself:**\n",
    "Having a better understanding of the data itself can often explain what is happening with the model. For example, knowing what percentage of people fell into each catagory for each scinario could potentially add more of an explination to what is seen in the model's output. This could help because in scinerio 1 part 1 the model predicted the correct answer about 85% of the time but if about 85% of people all fell within the same catagory then the high accuracy rate might not be as impressive. Below is the code that calculates the percentage of people that fall into each catagory for each scinario."
   ],
   "id": "27fd437fde152836"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_percentages(full_data):\n",
    "    n = len(full_data)\n",
    "    classed_arr = [np.argmax(sample) for sample in full_data]\n",
    "    n0, n1, n2 = classed_arr.count(0), classed_arr.count(1), classed_arr.count(2)\n",
    "    return (f\"Catagory 0: {round((n0-3)/n*100,2)}% Catagory 1: \"\n",
    "            f\"{round((n1-3)/n*100,2)}% Catagory 2: {round((n2-3)/n*100,2)}%\")\n",
    "\n",
    "names = itterate_situations_and_parts()\n",
    "for i in range(10):\n",
    "    print(f\"{names[i]} = {get_percentages(all_situations[i])}\")"
   ],
   "id": "958bb486448808f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Connecting to model output:**\n",
    "It is clear that a large majority of the samples (people) made decisions that fell into the thrid catagory. Most of them are near the same percentage of the model accuracy. That means if the model just predicted the sample making a decision that falls into catagory three then the output would be simalar. With that being said, S3P1 and S3P2 are both signifigently lower than the model's output. There are two things that come to mind. Number one is to change the catagories so instead of a 4-3-4 split it is a 3-5-3 split. This would potentally lower the percentage of samples that fall into catagory three. Another idea would be to build a decision tree model instead of a deep nerual network mode. This could work because decision threes are better for smaller and less complecated data. Changing the split to 3-5-3 is an easy change so I will do that first and see what happens. Below is the code to make the split. "
   ],
   "id": "cb194f17324f4f6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def classiy_and_catigorize(column):\n",
    "    return to_categorical([0 if x <= 2 else 1 if x <= 7 else 2 for x in column])\n",
    "\n",
    "S1P1, S1P2, S2P1, S2P2, S3P1, S3P2, S4P1, S4P2, S5P1, S5P2 = [classiy_and_catigorize(col) for col in columns]\n",
    "all_situations = [S1P1, S1P2, S2P1, S2P2, S3P1, S3P2, S4P1, S4P2, S5P1, S5P2]\n",
    "\n",
    "for situation in all_situations:\n",
    "    print(situation.shape)"
   ],
   "id": "460f04ed80b48255",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Dysplay ratios for each catagory**    ",
   "id": "f3c42c493b40f130"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range(10):\n",
    "    print(f\"{names[i]} = {get_percentages(all_situations[i])}\")"
   ],
   "id": "2fa997b07dde1ae8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **These ratios are better and will hopefully add more insight to what is happening**",
   "id": "88517c7d3a276382"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Split into training and validating:**\n",
    "Just as before the data with the new split need to be sorted into training and validating data. Below is the code for this."
   ],
   "id": "a8ba0e1ebb0387d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "S1P1_train, S1P1_val = split(S1P1)\n",
    "S1P2_train, S1P2_val = split(S1P2)\n",
    "S2P1_train, S2P1_val = split(S2P1)\n",
    "S2P2_train, S2P2_val = split(S2P2)\n",
    "S3P1_train, S3P1_val = split(S3P1)\n",
    "S3P2_train, S3P2_val = split(S3P2)\n",
    "S4P1_train, S4P1_val = split(S4P1)\n",
    "S4P2_train, S4P2_val = split(S4P2)\n",
    "S5P1_train, S5P1_val = split(S5P1)\n",
    "S5P2_train, S5P2_val = split(S5P2)\n",
    "\n",
    "print(f\"S1P1 training shape: {S1P1_train.shape}\")\n",
    "print(f\"Validation set length: {len(S1P1_val)}\")"
   ],
   "id": "ef958c97e3615536",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Train the model with new data split:**\n",
    "The model archetecture does not need to be rebuilt but the model does need to be reset and re-compiled."
   ],
   "id": "1e6aa2b13fcf2f52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = Model(inputs=[input1, input2], outputs=outputs, name='CustomizedDeepNeuralNetwork')\n",
    "\n",
    "metrics = ['accuracy'] * 10  \n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=metrics)\n",
    "\n",
    "print(model.summary())"
   ],
   "id": "2e04995f816cd535",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Train the model same way as before**",
   "id": "3b9e47b38f7e4536"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.fit([layer1_X_train, layer2_X_train], [S1P1_train, S1P2_train, S2P1_train,\n",
    "                    S2P2_train, S3P1_train, S3P2_train, S4P1_train,\n",
    "                    S4P2_train, S5P1_train, S5P2_train],\n",
    "          epochs=10,\n",
    "          batch_size=32,\n",
    "          validation_data=([layer1_X_val, layer2_X_val], [S1P1_val, S1P2_val, S2P1_val,\n",
    "                                                          S2P2_val, S3P1_val, S3P2_val, S4P1_val,\n",
    "                                                          S4P2_val, S5P1_val, S5P2_val]))"
   ],
   "id": "dfb1036e9d0ba5ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Understanding model output**\n",
    "The output again apears to suggest that the model is simply picking catagory three most of the time and that is why the percentages of accuracy is close to that of the ratio split. This could be because of poor model archetecture but I think trying out a decision tree next is a valid idea."
   ],
   "id": "8647a4144ecbcbbd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Decision Tree**\n",
    "A valid next move is to use a decision tree. This is likely going to work better because decision trees work very well with smaller less complicated data. It is possible the above model is too complecated to solve this problem... or not complicated enough but trying out a decision tree will provide more insight."
   ],
   "id": "6cdc6cd3dba1ebb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "dtc.fit(layer1_X_train, S1P1_train)\n",
    "\n",
    "predictions = dtc.predict(layer1_X_val)\n",
    "accuracy = accuracy_score(S1P1_val, predictions)\n",
    "for i in range(len(predictions)):\n",
    "    print(f\"{S1P1_val[i]} -> {predictions[i]}\")\n",
    "print(f\"Accuracy: {round(accuracy * 100, 2)}%\")\n"
   ],
   "id": "edba339b8d12e1ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c68f301416c3c42b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
